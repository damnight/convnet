net = network;

net.numInputs = 1; %one 32x32 preProc image (gabor energy map)
net.numLayers = 7; 

%biases (E5, and E4 may be realised this way)

%input connections: inputConnect(layer,input)
%input -> 1,2,3,5
net.inputConnect(1,1) = 1;
net.inputConnect(1,2) = 1;
net.inputConnect(1,3) = 1;
net.inputConnect(1,5) = 1;


%layer connections: layerConnect(targetLayer.sourceLayer)
%1 -> 5,3
%rest consecutive
net.layerConnect(5.1) = 1;
net.layerConnect(5.3) = 1;
net.layerConnect(3.2) = 1;
net.layerConnect(4.3) = 1;
net.layerConnect(5.4) = 1;
net.layerConnect(6.5) = 1;


%output connections: outputConnect [layer1 layer2 layer3] (indicate with
%boolean)
net.outputConnect = [0 0 0 0 0 1]; %layer 6 connects to output

%preprocessing functions for the inputs: inputs{layerIndex}.processFcns
net.inputs{1}.processFcns = {'preprocessImage'};


%size = #ofNeurons
%transferFcn = functionToNextLayer
%initFcn = initialization function
%set first layer neurons: surrWeighting
net.layers{1}.size = 32; %one neuron per pixel?
net.layers{1}.transferFcn = 'WDoG'; %part of the weighting function Ws*WDoG = Wside (11)
net.layers{1}.initFcn = 'initwb'; %https://de.mathworks.com/help/nnet/ref/init.html

%set second layer neurons: maxpooling to 28x28
net.layers{2}.size = 28; %one neuron per pixel?
net.layers{2}.transferFcn = ''; %endregion functions
net.layers{2}.initFcn = 'initwb'; %https://de.mathworks.com/help/nnet/ref/init.html

%set third layer neurons: adptEndWeighting
net.layers{3}.size = 8; %1/4 of previous neurons or maybe 4 times as many? really no clue atp
net.layers{3}.transferFcn = '';
net.layers{3}.initFcn = 'initnw';

%set fourth layer neurons: extAdptEndWeighting
net.layers{4}.size = 8; 
net.layers{4}.transferFcn = 
net.layers{4}.initFcn = 
%set fifth layer neurons: E-surround-end
net.layers{5}.size = 
net.layers{5}.transferFcn = 
net.layers{5}.initFcn = 

%set sixth layer neurons: binarization
net.layers{6}.size = 
net.layers{6}.transferFcn = 
net.layers{6}.initFcn =
%initialization, training, evaluation
net.initFcn = 'initlay'; %https://de.mathworks.com/help/nnet/ref/initlay.html?searchHighlight=initlay&s_tid=doc_srchtitle
net.trainFcn = 'trainlm'; %not sure if best training function
net.performFcn = 'mse'; %not sure it's usable since there must be layer in which it's tested, which currently resides outside the net


%initialize imds
    naturalImagesFolder = 'D:\User\Marco\Documents\!Studium\Informatik\VIP\matlab\convnet\images';
    %naturalImagesFolder = 'D:\User\Marco\Documents\!Studium\Informatik\VIP\matlab\convnet\images';
    natFilePattern = fullfile(naturalImagesFolder, '*.pgm');
    natFiles = dir(natFilePattern);
    baseFileName = natFiles().name;
    fullFileName = fullfile(naturalImagesFolder, baseFileName);
    %imds = imageDatastore(natFilePattern, 'LabelSource', 'foldernames');  
    imds = imageDatastore(fullfile(naturalImagesFolder, baseFileName), 'LabelSource', 'foldernames');
    %DEBUG 
    %disp(size(imds));    
    imds.ReadFcn = @(fullFileName)preprocessImage(fullFileName);
        Iout = preprocessImage(fullFileName);    

    %0. training options
    opts = trainingOptions('sgdm', ...
    'InitialLearnRate', 0.001, ...
   'LearnRateSchedule', 'piecewise', ...
    'LearnRateDropFactor', 0.1, ...
   'LearnRateDropPeriod', 8, ...
    'L2Regularization', 0.004, ...
    'MaxEpochs', 10, ...
    'MiniBatchSize', 100, ...
    'Verbose',true);
    
    
    %1. train cnn
    net = init(net);
    net = train(net,imds);
    view(net);

  
    %3. analyise net
    act1 = activations(net,imds,'conv_1','OutputAs','channels');
    sz = size(act1);
    %act1 = reshape(act1,[sz(1) sz(2) 1 sz(3)]);
    disp(size(act1));
    imshow(act1(:,:,1,4)); %does it display something good or not?
    %montage(mat2gray(act1),'Size',[8 12]);